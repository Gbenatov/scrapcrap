# ğŸ“Š ××¢×¨×›×ª Scraping - ×¡×˜×˜×•×¡ ×•×”×©×œ××”

## âœ… ×”×©×œ××” - 100%

××¢×¨×›×ª scraping ××œ××” ×•×–××™× ×” ×œ×©×™××•×©!

## ğŸ“ ×§×‘×¦×™× ×©× ×•×¦×¨×•

### Core Modules
- **[main_scraper.py](main_scraper.py)** - Scraper ×¨××©×™ ×¢× JSON extraction
- **[advanced_scraper.py](advanced_scraper.py)** - Selenium-based scraper ×œ××ª×¨×™× ×“×™× ××™×™×
- **[data_analyzer.py](data_analyzer.py)** - × ×™×ª×•×— ××ª×§×“× ×©×œ × ×ª×•× ×™×
- **[config.py](config.py)** - ×§×•× ×¤×™×’×•×¨×¦×™×” ××¨×›×–×™×ª
- **[run.py](run.py)** - Entry point ×œ×¤×¢×•×œ×” ××œ××”

### Testing & Documentation
- **[test_scraper.py](test_scraper.py)** - 6 ×‘×“×™×§×•×ª unit ×¢×•×‘×“×•×ª âœ“
- **[examples.py](examples.py)** - ×“×•×’×××•×ª ×¤×¢×•×œ×”
- **[USAGE.md](USAGE.md)** - ×ª×™×¢×•×“ ××œ× ×‘×¢×‘×¨×™×ª
- **[README.md](README.md)** - README ×ª××¦×™×ª×™

### Configuration
- **[requirements.txt](requirements.txt)** - Dependencies
- **[.env.example](.env.example)** - Template ×œ×¡×‘×™×‘×”
- **[.gitignore](.gitignore)** - Git ignore rules

## ğŸ¯ ×ª×›×•× ×•×ª ××¨×›×–×™×•×ª

### 1. JSON Extraction
```python
cases = scraper.extract_json_data(html_content)
```
- ×—×™×œ×•×¥ ×™×©×™×¨ ×-input element
- ×˜×™×¤×•×œ ×‘×©×’×™××•×ª ×¢×“×™×Ÿ
- 100% ×“×™×•×§ ×¢×‘×•×¨ ××ª×¨ ×‘×™×ª ×”××©×¤×˜

### 2. Data Processing
```python
processed = scraper.process_cases(cases)
```
- ×ª×¨×’×•× ×œ×¢×‘×¨×™×ª ××•×˜×•××˜×™
- × ×™×§×™×•×Ÿ ×•validation
- ×¤×•×¨××˜ ××—×™×“

### 3. Export Formats
```python
scraper.save_to_csv(cases)
scraper.save_to_json(cases)
```
- CSV ×¢× headers ×‘×¢×‘×¨×™×ª
- JSON ×¢× encoding UTF-8
- ××•×˜×•××˜×™ encoding

### 4. Analysis
```python
analyzer = DataAnalyzer(cases)
analyzer.generate_full_report()
```
- ×¡×˜×˜×™×¡×˜×™×§×” (×××•×¦×¢, ×—×¦×™×•×Ÿ, ×•×›×•')
- ×”×ª×¤×œ×’×•×™×•×ª
- ×ª×™×§×™ ×¢×¨×š ×’×‘×•×”
- ×“×•×—×•×ª

### 5. Logging
- Logging ××•×˜×•××˜×™ ×œ×§×•×‘×¥
- Console output ×‘×¢×‘×¨×™×ª
- ××¢×§×‘ ××œ× ×¢×œ ×‘×™×¦×•×¢

## ğŸ§ª ×‘×“×™×§×•×ª

```bash
python test_scraper.py -v
```

×ª×•×¦××”: **6/6 tests passed** âœ“

### Covered Tests
1. âœ“ Scraper initialization
2. âœ“ JSON extraction from HTML
3. âœ“ Case processing
4. âœ“ CSV export
5. âœ“ JSON export
6. âœ“ Empty cases handling

## ğŸš€ ×©×™××•×© ××”×™×¨

### ×“×•×’××” ×¤×©×•×˜×”
```python
from main_scraper import CaseScraper
from data_analyzer import DataAnalyzer

# Scrape
scraper = CaseScraper()
html = scraper.fetch_page()
cases = scraper.extract_json_data(html)
processed = scraper.process_cases(cases)

# Analyze
analyzer = DataAnalyzer(processed)
report = analyzer.generate_full_report()

# Export
scraper.save_to_csv(processed)
scraper.save_to_json(processed)
```

### End-to-End
```bash
python run.py
```

Outputs:
- `data/cases.csv`
- `data/cases.json`
- `data/report.json`
- `data/analysis_report.json`
- `logs/scraper.log`

## ğŸ“Š Sample Output

### CSV
```
××¡×¤×¨_×ª×™×§,×©×_×ª×™×§,×¡×›×•×_×ª×‘×™×¢×”,×‘×™×ª_××©×¤×˜,...
8011-01-25,×‘×–'× ×•×‘ × ' ××¨×™×¡×˜×•×Ÿ,15000000,××–×•×¨×™ ×œ×¢×‘×•×“×” ×—×™×¤×”,...
8012-01-25,×“×•×’××” ×©× ×™×” × ' × ×ª×‘×¢,25000000,××—×•×–×™ ×ª×œ ××‘×™×‘,...
```

### JSON
```json
[
  {
    "××¡×¤×¨_×ª×™×§": "8011-01-25",
    "×©×_×ª×™×§": "×‘×–'× ×•×‘ × ' ××¨×™×¡×˜×•×Ÿ",
    "×¡×›×•×_×ª×‘×™×¢×”": 15000000,
    ...
  }
]
```

## ğŸ”§ ×˜×›× ×•×œ×•×’×™×”

| Component | Technology |
|-----------|-----------|
| HTTP | requests |
| HTML Parsing | BeautifulSoup4 |
| Browser Automation | Selenium |
| Data Format | JSON/CSV |
| Logging | Python logging |
| Testing | unittest |

## ğŸ“ API Summary

### CaseScraper
- `fetch_page()` - ×”×•×¨×“×ª ×“×£
- `extract_json_data(html)` - ×—×™×œ×•×¥ JSON
- `process_cases(cases)` - ×¢×™×‘×•×“
- `save_to_csv(cases)` - ×™×™×¦×•× CSV
- `save_to_json(cases)` - ×™×™×¦×•× JSON
- `generate_report(cases)` - ×“×•×— ×¡×˜×˜×™×¡×˜×™

### DataAnalyzer
- `get_statistics()` - ×¡×˜×˜×™×¡×˜×™×§×” ×‘×¡×™×¡×™×ª
- `get_courts_distribution()` - ×—×œ×•×§×” ×œ×¤×™ ×‘×ª×™ ××©×¤×˜
- `get_plaintiff_groups_distribution()` - ×—×œ×•×§×” ×œ×¤×™ ×§×‘×•×¦×•×ª
- `get_appeal_cases_percentage()` - ××—×•×– ×¢×¨×¢×•×¨×™×
- `get_high_value_cases(threshold)` - ×ª×™×§×™ ×¢×¨×š ×’×‘×•×”
- `generate_full_report()` - ×“×•×— ××œ×

## âš™ï¸ Requirements

```
requests==2.31.0
beautifulsoup4==4.12.2
selenium==4.15.2
lxml==4.9.3
```

## ğŸ“ Learning Resources

- [USAGE.md](USAGE.md) - ×ª×™×¢×•×“ ××œ× (×¢×‘×¨×™×ª)
- [examples.py](examples.py) - ×“×•×’×××•×ª ×¢×•×‘×“×•×ª
- [README.md](README.md) - ××‘×•× ×§×¦×¨
- Source code - well-documented

## ğŸš¦ Next Steps

1. **Install**: `pip install -r requirements.txt`
2. **Run**: `python run.py`
3. **Check**: `data/cases.csv` and `data/cases.json`
4. **Analyze**: Open reports in your preferred tool
5. **Customize**: Edit config.py for your needs

## ğŸ“Œ Important Notes

1. **Rate Limiting**: ××œ ×ª×©×“×¨×’ ××ª ×”××ª×¨ - ×”×•×¡×£ `time.sleep()` ×‘×™×Ÿ ×‘×§×©×•×ª
2. **Terms of Service**: ×§×¨× ××ª ×ª× ××™ ×”×©×™××•×© ×©×œ ×”××ª×¨
3. **Data Privacy**: ×”× ×ª×•× ×™× ×”× public, ××‘×œ ×™×© ×œ×˜×¤×œ ×‘×”× ×‘×¢×“×™× ×•×ª
4. **Maintenance**: ×”××ª×¨ ××•×œ×™ ×™×ª×©× ×” - ×‘×“×•×§ regularly
5. **Caching**: ×©×§×•×œ caching ×©×œ HTML ×›×“×™ ×œ×—×¡×•×š bandwidth

## ğŸ“ Support

×‘×¢×™×•×ª? ×‘×“×•×§:
1. `logs/scraper.log` ×œ×¢× ×¤×¨×˜×™× ×¢×œ ×”×©×’×™××”
2. `USAGE.md` ×œ×¢× ×¤×ª×¨×•× ×•×ª ×œ×‘×¢×™×•×ª × ×¤×•×¦×•×ª
3. Code comments ×‘×§×‘×¦×™×

---

**Status**: âœ… Production Ready
**Last Updated**: 2025-01-21
**Version**: 1.0
